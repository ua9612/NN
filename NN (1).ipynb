{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pd.read_pickle(\"data_batch_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYS = list(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(dic[b'data'])\n",
    "y = list(dic[b'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train , dtype = np.float128).reshape(10000,1024)\n",
    "y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y:\n",
    "    temp = [0 for _ in range(10)]\n",
    "    temp[i] = 1\n",
    "    y_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train,dtype = np.float128).reshape(10000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 61.,  44.,  48., ..., 188., 124.,  99.], dtype=float128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x) *(1-sigmoid (x))\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x,wh,wo,bh,bo):\n",
    "    zh = np.dot(wh.T,x) + bh\n",
    "##  2x1 = 2x1024 X 1024x1\n",
    "    H = sigmoid(zh)\n",
    "    \n",
    "    \n",
    "    zo = np.dot(wo.T,H) + bo\n",
    "##  10x1  = 10x2 x 2x1\n",
    "\n",
    "    yhat = softmax(zo)\n",
    "##  10x1\n",
    "    cache = {\n",
    "        \"zh\":zh,\n",
    "        \"H\":H,\n",
    "        \"zo\":zo\n",
    "    }\n",
    "    return yhat , cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(x,y,wh,wo,lr):\n",
    "    yhat , cache = forward_prop(x,wh,wo,bh,bo)\n",
    "##  10x1\n",
    "    H = cache['H']\n",
    "##  2x1   \n",
    "    eo = yhat-y\n",
    "##  10x1 = 10x1 - 10x1    \n",
    "    eh = wo @ eo\n",
    "##  2x1 = 2x10 X 10x1   \n",
    "    dwo = H @ eo.T\n",
    "##  2x10 = 2x1 x 1x10  -->   eo = 10x1\n",
    "    dwh = x @ eh.T\n",
    "##  1024x2 = 1024x1 X 1x2 --> eh = 2x1\n",
    "    return dwh,dwo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(dwh, dwo , alpha , iters , wh, wo):\n",
    "    for i in range(iters):\n",
    "        wh -= alpha*dwh\n",
    "        wo -= alpha*dwo\n",
    "    return wh,wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "0\n",
      "4\n",
      "9\n",
      "0\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "8\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "5\n",
      "8\n",
      "9\n",
      "0\n",
      "0\n",
      "6\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "6\n",
      "8\n",
      "5\n",
      "5\n",
      "0\n",
      "7\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "6\n",
      "5\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "5\n",
      "1\n",
      "0\n",
      "5\n",
      "1\n",
      "5\n",
      "0\n",
      "9\n",
      "5\n",
      "5\n",
      "6\n",
      "3\n",
      "2\n",
      "4\n",
      "5\n",
      "5\n",
      "1\n",
      "0\n",
      "4\n",
      "3\n",
      "0\n",
      "5\n",
      "7\n",
      "0\n",
      "1\n",
      "3\n",
      "5\n",
      "6\n",
      "4\n",
      "1\n",
      "5\n",
      "6\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "4\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "4\n",
      "2\n",
      "5\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "8\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "2\n",
      "0\n",
      "9\n",
      "7\n",
      "5\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "2\n",
      "0\n",
      "5\n",
      "8\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "2\n",
      "0\n",
      "5\n",
      "8\n",
      "0\n",
      "5\n",
      "6\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "5\n",
      "3\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "3\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "0\n",
      "0\n",
      "6\n",
      "3\n",
      "5\n",
      "0\n",
      "0\n",
      "5\n",
      "9\n",
      "8\n",
      "6\n",
      "3\n",
      "5\n",
      "6\n",
      "0\n",
      "9\n",
      "4\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "9\n",
      "2\n",
      "0\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "3\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "9\n",
      "5\n",
      "4\n",
      "0\n",
      "6\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "7\n",
      "7\n",
      "5\n",
      "0\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "0\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "0\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "5\n",
      "8\n",
      "5\n",
      "1\n",
      "0\n",
      "0\n",
      "7\n",
      "1\n",
      "0\n",
      "0\n",
      "5\n",
      "7\n",
      "4\n",
      "5\n",
      "5\n",
      "0\n",
      "9\n",
      "3\n",
      "0\n",
      "9\n",
      "5\n",
      "6\n",
      "5\n",
      "5\n",
      "2\n",
      "6\n",
      "5\n",
      "0\n",
      "8\n",
      "0\n",
      "5\n",
      "7\n",
      "2\n",
      "8\n",
      "5\n",
      "2\n",
      "7\n",
      "5\n",
      "5\n",
      "0\n",
      "5\n",
      "8\n",
      "6\n",
      "5\n",
      "5\n",
      "3\n",
      "0\n",
      "0\n",
      "5\n",
      "5\n",
      "2\n",
      "0\n",
      "2\n",
      "5\n",
      "7\n",
      "0\n",
      "0\n",
      "9\n",
      "5\n",
      "9\n",
      "0\n",
      "0\n",
      "8\n",
      "3\n",
      "5\n",
      "0\n",
      "5\n",
      "0\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "6\n",
      "0\n",
      "0\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):   \n",
    "    i = -1\n",
    "    X = X_train[i,:].reshape(1024,1)\n",
    "    y = y_train[i,:].reshape(10,1)\n",
    "\n",
    "    wh = np.random.randn(1024,2) \n",
    "    wo = np.random.randn(2,10) \n",
    "    bh = np.full((2,1),1,dtype = np.float128)\n",
    "    bo = np.full((10,1),1 , dtype = np.float128)\n",
    "\n",
    "    alpha = 0.03\n",
    "    iters = 1000\n",
    "\n",
    "    y_hat , cache = forward_prop (X , wh , wo , bh , bo)\n",
    "    dwh,dwo = backward_prop(X,y,wh,wo,alpha)\n",
    "    wh,wo = gradientDescent(dwh, dwo , alpha, iters , wh , wo)\n",
    "\n",
    "    ##Final\n",
    "\n",
    "    ans , CACHE = forward_prop(X,wh,wo,bh,bo)\n",
    "    ##print(ans) \n",
    "    ##print(y)\n",
    "\n",
    "    value = max(ans)\n",
    "    i,j = np.where(np.isclose(ans, value))\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
